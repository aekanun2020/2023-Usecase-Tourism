{"cells": [{"cell_type": "raw", "id": "c452a577-837e-478e-b996-bff8e13c24a5", "metadata": {}, "source": "gcloud dataproc clusters create cluster-dbca --enable-component-gateway --bucket 23feb2023 --region us-central1 --zone us-central1-a --single-node --master-machine-type n2-standard-4 --master-boot-disk-size 500 --image-version 2.0-debian10 --optional-components JUPYTER --project tidal-eon-374408\n"}, {"cell_type": "code", "execution_count": 1, "id": "5a6f2cbc-3ff7-4751-ac47-0f31d0b7aa96", "metadata": {}, "outputs": [], "source": "import time as t"}, {"cell_type": "code", "execution_count": 2, "id": "6b098d1e-4713-415e-989f-a0751d48849f", "metadata": {}, "outputs": [], "source": "start_time = t.time()"}, {"cell_type": "code", "execution_count": 3, "id": "80f5fe4a-735c-464d-94a6-cccb60a5cf1f", "metadata": {}, "outputs": [], "source": "#! gsutil cp gs://23feb2023/tripadvisor/* ./"}, {"cell_type": "code", "execution_count": 4, "id": "df14bee7-4bff-44ca-9c48-b1d05fb19cc4", "metadata": {}, "outputs": [], "source": "#! sudo apt-get install p7zip"}, {"cell_type": "code", "execution_count": 5, "id": "6310ae13-1010-49b9-9f72-f1d664cf920a", "metadata": {}, "outputs": [], "source": "#! p7zip -d hotel-20211115.7z"}, {"cell_type": "code", "execution_count": 31, "id": "08cb31fd-084a-47ec-8746-baab9bf7fcf4", "metadata": {}, "outputs": [], "source": "#! ls -l /hotel/tripadvisor/review/"}, {"cell_type": "code", "execution_count": 6, "id": "75e932f1-cba2-4f82-8d04-3c23e90c278b", "metadata": {}, "outputs": [], "source": "#! gsutil -m cp -r /hotel/tripadvisor/review gs://23feb2023/tripadvisor/"}, {"cell_type": "code", "execution_count": 8, "id": "9daed8c3", "metadata": {}, "outputs": [{"data": {"text/html": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://cluster-dbca-m.us-central1-a.c.tidal-eon-374408.internal:40285\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        ", "text/plain": "<SparkContext master=yarn appName=PySparkShell>"}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": "sc"}, {"cell_type": "code", "execution_count": 9, "id": "2b7a5657", "metadata": {}, "outputs": [], "source": "from pyspark.sql import functions as sparkf\nfrom pyspark.sql.types import *"}, {"cell_type": "code", "execution_count": 10, "id": "c1dd91cb", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "loaded_df = spark.read.json('gs://23feb2023/tripadvisor/review/*.json')"}, {"cell_type": "code", "execution_count": 11, "id": "be03ac75", "metadata": {}, "outputs": [], "source": "#loaded_df.count()"}, {"cell_type": "code", "execution_count": 12, "id": "258ea10c", "metadata": {}, "outputs": [], "source": "#loaded_df.rdd.getNumPartitions()"}, {"cell_type": "code", "execution_count": 13, "id": "4cc99b43", "metadata": {}, "outputs": [], "source": "raw_df = loaded_df"}, {"cell_type": "raw", "id": "0f9f7919", "metadata": {}, "source": "raw_df = loaded_df.repartition(282)"}, {"cell_type": "code", "execution_count": 14, "id": "714cfaaa", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "analysis_df = raw_df.select('id','createdDate','additionalRatings'\\\n                            ,sparkf.col('location.additionalNames.geo').alias('location_geo')\\\n                            ,sparkf.col('location.placeType').alias('location_placeType')\\\n                            ,sparkf.col('location.name').alias('location_name')\\\n              ,'rating','userProfile.userId'\\\n                            ,'userProfile.hometown.location.name')\\\n.withColumnRenamed('rating','rating_col')\\\n.withColumnRenamed('name','userHometown')\\\n.withColumn('extracted_additionalRatings',sparkf.explode('additionalRatings'))\\\n.select(\n  'id','createdDate','additionalRatings','extracted_additionalRatings'\\\n    , sparkf.col(\"extracted_additionalRatings\")[\"rating\"].alias(\"rating\")\\\n    , sparkf.col(\"extracted_additionalRatings\")[\"ratingLabel\"].alias(\"ratingLabel\")\\\n    ,'location_geo','location_name','rating_col','userId','userHometown','location_placeType'\n)\\\n.groupBy('id','createdDate','rating_col','location_placeType','location_name','location_geo'\\\n         ,'userId','userHometown').pivot('ratingLabel').sum('rating')\\\n.orderBy('id', ascending= False)"}, {"cell_type": "code", "execution_count": 15, "id": "8fb451e7", "metadata": {}, "outputs": [], "source": "#analysis_df.count()"}, {"cell_type": "code", "execution_count": 16, "id": "c2770437", "metadata": {}, "outputs": [], "source": "sparkf_ReplaceNull = sparkf.udf(lambda x: \"UNKNOWN\" if x == None else x)"}, {"cell_type": "code", "execution_count": 17, "id": "b410c2db", "metadata": {}, "outputs": [], "source": "final_df = analysis_df\\\n.withColumnRenamed('Business service (e.g., internet access)','Business_service')\\\n.withColumn('userHometown',sparkf_ReplaceNull('userHometown'))\\\n.withColumn('Cleanliness',sparkf.col('Cleanliness').cast(IntegerType()))\\\n.withColumn('Location',sparkf.col('Location').cast(IntegerType()))\\\n.withColumn('Rooms',sparkf.col('Rooms').cast(IntegerType()))\\\n.withColumn('Service',sparkf.col('Service').cast(IntegerType()))\\\n.withColumn('Sleep Quality',sparkf.col('Sleep Quality').cast(IntegerType()))\\\n.withColumn('Value',sparkf.col('Value').cast(IntegerType()))\\\n.withColumn('Business_service'\\\n            ,sparkf.col('Business_service').cast(IntegerType()))\\\n.withColumn('Check in / front desk',sparkf.col('Check in / front desk').cast(IntegerType()))\\\n.withColumn('createdDate',sparkf.to_timestamp('createdDate',format='yyyy-MM-dd'))\n"}, {"cell_type": "raw", "id": "70da63c8", "metadata": {}, "source": "final_df.write.option(\"header\",True).mode('overwrite').csv('gs://usjqbewjtps/export/29dec2021')"}, {"cell_type": "code", "execution_count": null, "id": "d7470c67", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 18, "id": "790bae21", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 14:==================================>                       (3 + 2) / 5]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------+-----+\n|Sleep Quality|count|\n+-------------+-----+\n|         null|33953|\n|            1| 1742|\n|            2| 1533|\n|            3| 4840|\n|            4|10490|\n|            5|19441|\n|            6|   54|\n|            8|  100|\n|           10|  138|\n+-------------+-----+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "final_df.groupBy('Sleep Quality').count().orderBy('Sleep Quality').show(100)"}, {"cell_type": "code", "execution_count": 19, "id": "54983974", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/02/26 13:43:03 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1677412593959_0005_01_000001 on host: cluster-dbca-m.us-central1-a.c.tidal-eon-374408.internal. Exit status: 137. Diagnostics: [2023-02-26 13:43:03.445]Container killed on request. Exit code is 137\n[2023-02-26 13:43:03.445]Container exited with a non-zero exit code 137. \n[2023-02-26 13:43:03.446]Killed by external signal\n.\n23/02/26 13:43:03 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 1 for reason Container from a bad node: container_1677412593959_0005_01_000001 on host: cluster-dbca-m.us-central1-a.c.tidal-eon-374408.internal. Exit status: 137. Diagnostics: [2023-02-26 13:43:03.445]Container killed on request. Exit code is 137\n[2023-02-26 13:43:03.445]Container exited with a non-zero exit code 137. \n[2023-02-26 13:43:03.446]Killed by external signal\n.\n23/02/26 13:43:03 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 1 on cluster-dbca-m.us-central1-a.c.tidal-eon-374408.internal: Container from a bad node: container_1677412593959_0005_01_000001 on host: cluster-dbca-m.us-central1-a.c.tidal-eon-374408.internal. Exit status: 137. Diagnostics: [2023-02-26 13:43:03.445]Container killed on request. Exit code is 137\n[2023-02-26 13:43:03.445]Container exited with a non-zero exit code 137. \n[2023-02-26 13:43:03.446]Killed by external signal\n.\n23/02/26 13:43:03 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 9.0 in stage 19.0 (TID 9046) (cluster-dbca-m.us-central1-a.c.tidal-eon-374408.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1677412593959_0005_01_000001 on host: cluster-dbca-m.us-central1-a.c.tidal-eon-374408.internal. Exit status: 137. Diagnostics: [2023-02-26 13:43:03.445]Container killed on request. Exit code is 137\n[2023-02-26 13:43:03.445]Container exited with a non-zero exit code 137. \n[2023-02-26 13:43:03.446]Killed by external signal\n.\n23/02/26 13:43:03 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 11.0 in stage 19.0 (TID 9048) (cluster-dbca-m.us-central1-a.c.tidal-eon-374408.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1677412593959_0005_01_000001 on host: cluster-dbca-m.us-central1-a.c.tidal-eon-374408.internal. Exit status: 137. Diagnostics: [2023-02-26 13:43:03.445]Container killed on request. Exit code is 137\n[2023-02-26 13:43:03.445]Container exited with a non-zero exit code 137. \n[2023-02-26 13:43:03.446]Killed by external signal\n.\n[Stage 24:==================================>                       (3 + 2) / 5]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-------+-----+\n|Service|count|\n+-------+-----+\n|   null|  375|\n|      1| 4065|\n|      6|  101|\n|      3| 8033|\n|      5|39081|\n|      4|17131|\n|      8|  143|\n|     10|  307|\n|      2| 3055|\n+-------+-----+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "final_df.groupBy('Service').count().show(100)"}, {"cell_type": "code", "execution_count": 20, "id": "1cdc3f60", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- id: long (nullable = true)\n |-- createdDate: timestamp (nullable = true)\n |-- rating_col: long (nullable = true)\n |-- location_placeType: string (nullable = true)\n |-- location_name: string (nullable = true)\n |-- location_geo: string (nullable = true)\n |-- userId: string (nullable = true)\n |-- userHometown: string (nullable = true)\n |-- Business_service: integer (nullable = true)\n |-- Check in / front desk: integer (nullable = true)\n |-- Cleanliness: integer (nullable = true)\n |-- Location: integer (nullable = true)\n |-- Rooms: integer (nullable = true)\n |-- Service: integer (nullable = true)\n |-- Sleep Quality: integer (nullable = true)\n |-- Value: integer (nullable = true)\n\n"}], "source": "final_df.printSchema()"}, {"cell_type": "raw", "id": "1ad585d2", "metadata": {}, "source": "! gsutil rm gs://usjqbewjtps/export/29dec2021/_SUCCESS"}, {"cell_type": "code", "execution_count": 21, "id": "b65cf748", "metadata": {}, "outputs": [], "source": "# Use the Google Cloud Storage bucket for temporary BigQuery export data used\n# by the InputFormat. This assumes the Google Cloud Storage connector for\n# Hadoop is configured.\nbucket = sc._jsc.hadoopConfiguration().get('fs.gs.system.bucket')\nproject = sc._jsc.hadoopConfiguration().get('fs.gs.project.id')\ninput_directory = 'gs://{}/hadoop/tmp/bigquery/pyspark_input'.format(bucket)\n"}, {"cell_type": "code", "execution_count": 22, "id": "a25cf615", "metadata": {}, "outputs": [{"data": {"text/plain": "'23feb2023'"}, "execution_count": 22, "metadata": {}, "output_type": "execute_result"}], "source": "bucket"}, {"cell_type": "code", "execution_count": 23, "id": "8b581016", "metadata": {}, "outputs": [{"data": {"text/plain": "'tidal-eon-374408'"}, "execution_count": 23, "metadata": {}, "output_type": "execute_result"}], "source": "project"}, {"cell_type": "code", "execution_count": 24, "id": "227dc3cf", "metadata": {}, "outputs": [], "source": "# Output Parameters.\noutput_dataset = 'tourism_dataset'\noutput_table = 'tourism_table'"}, {"cell_type": "code", "execution_count": 25, "id": "86b5d828", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Stage data formatted as newline-delimited JSON in Google Cloud Storage.\noutput_directory = 'gs://{}/hadoop/tmp/bigquery/pyspark_output'.format(bucket)\n#partitions = range(word_counts.getNumPartitions())\noutput_files = output_directory + '/part-*'\n\n\noutput_files\n\nfinal_df.write.option(\"header\",True).mode('overwrite').format('csv').save(output_directory)\n"}, {"cell_type": "code", "execution_count": 26, "id": "582deacf", "metadata": {}, "outputs": [{"data": {"text/plain": "'gs://23feb2023/hadoop/tmp/bigquery/pyspark_output/part-*'"}, "execution_count": 26, "metadata": {}, "output_type": "execute_result"}], "source": "output_files"}, {"cell_type": "code", "execution_count": 27, "id": "b08d275b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "   2665588  2023-02-26T13:48:18Z  gs://23feb2023/hadoop/tmp/bigquery/pyspark_output/part-00000-720f16c5-afc7-49b4-8971-ac58195a635c-c000.csv\n   2520200  2023-02-26T13:48:18Z  gs://23feb2023/hadoop/tmp/bigquery/pyspark_output/part-00001-720f16c5-afc7-49b4-8971-ac58195a635c-c000.csv\n   2529990  2023-02-26T13:48:18Z  gs://23feb2023/hadoop/tmp/bigquery/pyspark_output/part-00002-720f16c5-afc7-49b4-8971-ac58195a635c-c000.csv\n   2566194  2023-02-26T13:48:18Z  gs://23feb2023/hadoop/tmp/bigquery/pyspark_output/part-00003-720f16c5-afc7-49b4-8971-ac58195a635c-c000.csv\n     98674  2023-02-26T13:48:20Z  gs://23feb2023/hadoop/tmp/bigquery/pyspark_output/part-00004-720f16c5-afc7-49b4-8971-ac58195a635c-c000.csv\nTOTAL: 5 objects, 10380646 bytes (9.9 MiB)\n"}], "source": "! gsutil ls -l gs://23feb2023/hadoop/tmp/bigquery/pyspark_output/part-*"}, {"cell_type": "code", "execution_count": 28, "id": "3726d557-602c-4aab-9cd2-80ff896f3e26", "metadata": {}, "outputs": [{"data": {"text/plain": "1036.0935039520264"}, "execution_count": 28, "metadata": {}, "output_type": "execute_result"}], "source": "t.time()-start_time"}, {"cell_type": "code", "execution_count": 29, "id": "2a784666", "metadata": {}, "outputs": [], "source": "import subprocess"}, {"cell_type": "code", "execution_count": 30, "id": "396f5fb7", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Waiting on bqjob_rd3ffca51837e3dd_000001868dfb5c37_1 ... (2s) Current status: DONE   \n"}, {"data": {"text/plain": "0"}, "execution_count": 30, "metadata": {}, "output_type": "execute_result"}], "source": "# Shell out to bq CLI to perform BigQuery import.\nsubprocess.check_call(\n    'bq load --source_format=CSV  '\n    '--replace '\n    '--autodetect '\n    '{dataset}.{table} {files} '.format(\n        dataset=output_dataset, table=output_table, files=output_files\n    ).split())"}, {"cell_type": "code", "execution_count": null, "id": "b53849ed", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}